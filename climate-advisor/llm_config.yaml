# Climate Advisor LLM Configuration
# This file contains all LLM-related configuration for the Climate Advisor service

# Model Configuration
models:
  # Default model to use when none is specified
  default: "openai/gpt-4.1"

  # Available models with their capabilities and default parameters
  available:
    "openai/gpt-4.1":
      name: "GPT-4.1"
      default_temperature: 0.2

# Generation Parameters
generation:
  # Default parameters for text generation
  defaults:
    temperature: 0.1

# System Prompts
prompts:
  # Default system prompt for general climate advice
  default: "prompts/default.md"

  # Prompt for inventory-specific context
  inventory_context: "prompts/inventory_context.md"

  # Prompt for data analysis tasks
  data_analysis: "prompts/data_analysis.md"

# API Configuration
api:
  # OpenRouter configuration
  openrouter:
    base_url: "https://openrouter.ai/api/v1"
    # API key should be set via environment variable OPENROUTER_API_KEY
    timeout_ms: 30000
    retry_attempts: 3
    retry_delay_ms: 1000

  # OpenAI configuration for embeddings
  openai:
    base_url: "https://api.openai.com/v1"
    # API key should be set via environment variable OPENAI_API_KEY
    timeout_ms: 30000
    embedding_model: "text-embedding-3-large"

  # Request configuration
  requests:
    # Maximum time to wait for a response
    timeout_ms: 30000
    # Maximum number of retries for failed requests
    max_retries: 3
    # Delay between retries (exponential backoff)
    retry_delay_ms: 1000

# Tool Configuration
tools:
  # Climate Vector Search Tool
  climate_vector_search:
    enabled: true
    top_k: 3
    min_score: 0.6
    timeout_seconds: 20
    description: "Search the climate knowledge base for relevant information about climate change, emissions, GHG, carbon, sustainability, environmental policies, renewable energy, net zero goals, climate adaptation, and mitigation strategies."

# Conversation Configuration
conversation:
  # Number of previous messages to include for context
  history_limit: 5
  # Whether to include conversation history in agent context
  include_history: true

  # Conversation History Retention & Pruning
  retention:
    # Number of most recent turns to preserve in LLM context
    # A "turn" = user message + assistant response
    # NOTE:
    # - Tool metadata (`messages.tools_used`) is always stored in DB.
    # - It is NOT sent as extra keys in Responses API input items (unknown fields are rejected).
    # - Recent tool outputs are injected as additional SYSTEM messages (role/content only)
    #   for preserved turns so follow-up requests stay grounded (e.g. remembering inventoryId).
    preserve_turns: 10

    # Maximum number of messages to load from database per request
    # Older messages beyond this limit are not loaded at all
    max_loaded_messages: 20

    # Whether to apply pruning logic for history payload size (message-count based)
    # If true: only the last preserve_turns are treated as "preserved" in logs/metrics.
    # If false: all loaded messages are treated as preserved (still role/content only).
    prune_tools_for_llm: true

# Feature Flags
features:
  # Enable/disable streaming responses
  streaming_enabled: true

# Logging Configuration
logging:
  log_requests: true
  log_responses: true
  log_performance: true

# Observability Configuration
observability:
  langsmith:
    # Non-secret defaults for LangSmith integration
    project: "climate_advisor"
    endpoint: "https://api.smith.langchain.com"
    tracing_enabled: true
